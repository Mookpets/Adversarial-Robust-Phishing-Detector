{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udd17 Fine-Tune BERT for Phishing Email Detection\n", "Using the [TanvirAhammed/phishing_email_dataset](https://huggingface.co/datasets/TanvirAhammed/phishing_email_dataset) from Hugging Face.\n", "\n", "**Steps:**\n", "1. Load the dataset\n", "2. Preprocess and tokenize\n", "3. Fine-tune BERT\n", "4. Evaluate the model"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Install Hugging Face libraries\n", "!pip install transformers datasets accelerate -q"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Step 1: Load the dataset\n", "from datasets import load_dataset\n", "dataset = load_dataset(\"TanvirAhammed/phishing_email_dataset\")\n", "dataset"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Step 2: Tokenize using BERT tokenizer\n", "from transformers import AutoTokenizer\n", "\n", "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n", "\n", "def tokenize_function(example):\n", "    return tokenizer(example['text'], padding=\"max_length\", truncation=True)\n", "\n", "tokenized_datasets = dataset.map(tokenize_function, batched=True)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Step 3: Prepare the training pipeline\n", "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n", "\n", "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n", "\n", "training_args = TrainingArguments(\n", "    output_dir=\"./results\",\n", "    evaluation_strategy=\"epoch\",\n", "    learning_rate=2e-5,\n", "    per_device_train_batch_size=8,\n", "    per_device_eval_batch_size=8,\n", "    num_train_epochs=3,\n", "    weight_decay=0.01,\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_datasets['train'],\n", "    eval_dataset=tokenized_datasets['test'],\n", "    tokenizer=tokenizer,\n", ")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Step 4: Train the model\n", "trainer.train()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Step 5: Evaluate the model\n", "trainer.evaluate()"]}], "metadata": {"colab": {"name": "bert_phishing_detection_huggingface.ipynb"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 1}